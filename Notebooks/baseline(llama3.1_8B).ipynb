{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c1wj_w6IoCe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        text = alpaca_prompt.format(instruction, \"\", output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"Training Complete!\")\n",
        "\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "print(\"Model saved to 'lora_model' folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# 1. ×ž×¢×‘×¨ ×œ×ž×¦×‘ '×”×¡×§×”' (Inference Mode)\n",
        "# ×× ×—× ×• ×ž×©×ª×ž×©×™× ×‘×ž×•×“×œ ×©×›×‘×¨ × ×ž×¦× ×‘×–×™×›×¨×•×Ÿ ×ž×”×©×œ×‘ ×”×§×•×“×!\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 2. ×”×’×“×¨×ª ×”×¤×¨×•×ž×¤×˜ ×•×”×¤×•× ×§×¦×™×”\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def generate_response(context, question):\n",
        "    # ×”×›× ×ª ×”×¤×¨×•×ž×¤×˜\n",
        "    inputs = tokenizer(\n",
        "        [\n",
        "            alpaca_prompt.format(\n",
        "                \"You are a banking regulation compliance assistant. Based strictly on the provided context, answer the user's question in JSON format. Required fields: \\\"answer\\\" (Yes/No), \\\"citation\\\" (verbatim quote), \\\"explanation\\\" (concise justification).\",\n",
        "                f\"Context: {context}\\nQuestion: {question}\",\n",
        "                \"\",\n",
        "            )\n",
        "        ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "    decoded = tokenizer.batch_decode(outputs)\n",
        "\n",
        "    response = decoded[0].split(\"### Response:\\n\")[-1].replace(tokenizer.eos_token, \"\").strip()\n",
        "    return response\n",
        "\n",
        "\n",
        "if not os.path.exists(\"test_dataset.json\"):\n",
        "    print(\"âš ï¸ STOP! You must upload 'test_dataset.json' to the files sidebar first.\")\n",
        "else:\n",
        "\n",
        "    print(\"Loading test data...\")\n",
        "    with open(\"test_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        test_data = json.load(f)\n",
        "\n",
        "    results = []\n",
        "    print(f\"ðŸš€ Starting evaluation on {len(test_data)} examples...\")\n",
        "\n",
        "\n",
        "    for item in tqdm(test_data):\n",
        "        full_text = item['instruction']\n",
        "        try:\n",
        "\n",
        "            if \"Context: \" in full_text and \"\\nQuestion: \" in full_text:\n",
        "                context_part = full_text.split(\"Context: \")[1].split(\"\\nQuestion: \")[0]\n",
        "                question_part = full_text.split(\"\\nQuestion: \")[1]\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "\n",
        "            prediction = generate_response(context_part, question_part)\n",
        "\n",
        "            results.append({\n",
        "                \"question\": question_part,\n",
        "                \"context\": context_part,\n",
        "                \"true_answer\": item['output'],\n",
        "                \"model_prediction\": prediction\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Skipped an item due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "    with open(\"evaluation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(\"\\n Evaluation Done! Saved to 'evaluation_results.json'\")\n",
        "    print(\"Refresh the files sidebar to see the result file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdzwHFH6f53H",
        "outputId": "77451555-9a62-470d-bd4f-77afd57f0f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading test data...\n",
            "ðŸš€ Starting evaluation on 50 examples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:12<00:00,  5.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Evaluation Done! Saved to 'evaluation_results.json'\n",
            "Refresh the files sidebar to see the result file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}